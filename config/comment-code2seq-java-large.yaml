data_folder: ./dataset

checkpoint: null

seed: 7
# Training in notebooks (e.g. Google Colab) may crash with too small value
progress_bar_refresh_rate: 1
print_config: true

wandb:
  project: comment-code2seq
  group: null
  offline: true

data:
  num_workers: 4

  base_tokenizer: "microsoft/codebert-base"
  train_new_tokenizer: true
  max_tokenizer_vocab: 20000

  # Each token appears at least 10 times (99.2% coverage)
  labels_count: 10
  max_label_parts: 256
  # Each token appears at least 1000 times (99.5% coverage)
  tokens_count: 1000
  max_token_parts: 5
  path_length: 9

  max_context: 200
  random_context: true

  batch_size: 64
  test_batch_size: 64

model:
  # Encoder
  embedding_size: 128
  encoder_dropout: 0.25
  encoder_rnn_size: 128
  use_bi_rnn: true
  rnn_num_layers: 1

  # Decoder
  decoder_size: 512
  decoder_num_layers: 4
  decoder_dim_feedforward: 2048
  decoder_num_heads: 8
  decoder_dropout: 0.1

optimizer:
  optimizer: "Momentum"
  nesterov: true
  lr: 0.01
  weight_decay: 0
  decay_gamma: 0.95

train:
  gpu: 1
  n_epochs: 100
  patience: 10
  clip_norm: 5
  teacher_forcing: 1.0
  val_every_epoch: 1
  save_every_epoch: 1
  log_every_n_steps: 10
